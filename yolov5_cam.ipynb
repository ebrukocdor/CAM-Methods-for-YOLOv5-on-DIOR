{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sywiyc4hcdfX",
        "outputId": "a64ca750-bf8c-4903-e0a4-17714bc23b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPH50FpDcdnl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import gdown\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "from xml.etree import ElementTree\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJwcIO62jBQG",
        "outputId": "e7ef1da9-1f71-4831-c06e-2bea6f14960d"
      },
      "outputs": [],
      "source": [
        "!pip install yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKFV70kG62dg",
        "outputId": "fc3f9ba2-88d7-4c00-9738-00d0ed837bd9"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam==1.4.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlcGwCoCWeg5"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pytorch_grad_cam import (EigenCAM, GradCAM, GradCAMPlusPlus)\n",
        "from pytorch_grad_cam.utils.image import scale_cam_image, show_cam_on_image\n",
        "\n",
        "from yolov5.models.common import AutoShape, DetectMultiBackend\n",
        "from yolov5.utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages\n",
        "from yolov5.utils.general import LOGGER, check_file, check_img_size, increment_path, print_args, xywh2xyxy\n",
        "from yolov5.utils.torch_utils import select_device\n",
        "\n",
        "def yolo_reshape_transform(x):\n",
        "    \"\"\"\n",
        "    # The backbone outputs different tensors with different spatial sizes, from the FPN.\n",
        "    Our goal here is to aggregate these image tensors, assign them weights, and then aggregate everything.\n",
        "    To do that, we are going to need to write a custom function that takes these tensors with different sizes,\n",
        "    resizes them to a common shape, and concatenates them\n",
        "    https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html\n",
        "    it seems that output is always the same shape in yolo. So, this is not needed.\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "\n",
        "class YOLOBoxScoreTarget():\n",
        "    \"\"\"\n",
        "    This way we see all boxes.\n",
        "    then we filter out classes and select the classes that we want to attend to.\n",
        "    At the end, we sum out of all these.\n",
        "\n",
        "    This is not a standard approach. This is somewhat similar to what\n",
        "    https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "    has done.\n",
        "\n",
        "    Here the problem is that we are taking a lot of attention to overlapping boxes.\n",
        "    This should not be the case.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        self.classes = set(classes)\n",
        "\n",
        "    def __call__(self, output):\n",
        "        \"\"\"\n",
        "        # here we need something which we can call backward\n",
        "        https://pub.towardsai.net/yolov5-m-implementation-from-scratch-with-pytorch-c8f84a66c98b\n",
        "        output structure is taken from this tutorial, it is as follows:\n",
        "\n",
        "        first item is important, second item contains three arrays which contain prediction from three heads\n",
        "        we would use the first array as it is the final prediction.\n",
        "        pred = output[0]\n",
        "        Here, we take the first item as the second item contains predictions from three heads. Also, each head dimension would be different\n",
        "        as we have different dimensions per head.\n",
        "\n",
        "        \"xc,yc,height, width,objectness, classes\"\n",
        "        so, the forth item would be objectness and items after fifth element are class indexes\n",
        "        \"\"\"\n",
        "        if len(output.shape) == 2:\n",
        "            output = torch.unsqueeze(output, dim=0)\n",
        "\n",
        "        assert len(output.shape) == 3\n",
        "        classes = output[:, :, 5:]\n",
        "        mask = torch.zeros_like(classes, dtype=torch.bool)\n",
        "        for class_idx in self.classes:\n",
        "            mask[:, :, class_idx] = True\n",
        "\n",
        "        score = classes[mask]  \n",
        "        return score.sum()\n",
        "\n",
        "\n",
        "class YOLOBoxScoreTarget2():\n",
        "    \"\"\" # For every original detected bounding box specified in \"bounding boxes\",\n",
        "        assign a score on how the current bounding boxes match it,\n",
        "            1. In IOU\n",
        "            2. In the classification score.\n",
        "        If there is not a large enough overlap, or the category changed,\n",
        "        assign a score of 0.\n",
        "\n",
        "        The total score is the sum of all the box scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, predicted_bbox, backprop, classes, device):\n",
        "        \"\"\"\n",
        "        # Initializes the YOLOBoxScoreTarget2 module.\n",
        "\n",
        "        Args:\n",
        "            predicted_bbox: A tensor containing the predicted bounding box coordinates,\n",
        "                confidence scores, and class indices.\n",
        "            backprop: A string indicating which parameter to backpropagate through.\n",
        "            classes: A list of class indices to consider.\n",
        "        \"\"\"\n",
        "        self.predicted_bbox = predicted_bbox\n",
        "        self.backprop = backprop\n",
        "        self.classes = classes\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, output):\n",
        "        \"\"\"\n",
        "        here we need something which we can call backward\n",
        "        https://pub.towardsai.net/yolov5-m-implementation-from-scratch-with-pytorch-c8f84a66c98b\n",
        "        output structure is taken from this tutorial.\n",
        "\n",
        "        \"center_x, center_y, width, height,confidence, classes\"\n",
        "        so, the forth item would be confidence and items after fifth element are class indexes\n",
        "        \"\"\"\n",
        "        if len(output.shape) == 2:\n",
        "            output = torch.unsqueeze(output, dim=0)\n",
        "\n",
        "        assert len(output.shape) == 3\n",
        "        bboxes_processed = xywh2xyxy(output[..., :4])\n",
        "\n",
        "        iou_scores = torchvision.ops.box_iou(self.predicted_bbox[:, :4], bboxes_processed[0])\n",
        "        topk_iou_values, topk_iou_indices = iou_scores.topk(k=10, dim=-1)  \n",
        "\n",
        "        score = torch.tensor([0.0], requires_grad=True, device=self.device)\n",
        "\n",
        "        for i, (x1, y1, x2, y2, confidence, class_idx) in enumerate(self.predicted_bbox):\n",
        "            class_idx = int(class_idx)\n",
        "\n",
        "            if class_idx not in self.classes:\n",
        "                continue\n",
        "\n",
        "            indices, values = topk_iou_indices[i], topk_iou_values[i]\n",
        "\n",
        "            filtered_indices = output[0, indices, 5:].max(dim=1)[1] == class_idx\n",
        "            indices = indices[filtered_indices]\n",
        "            values = values[filtered_indices]\n",
        "\n",
        "            if len(indices.size()) == 0:\n",
        "                continue\n",
        "\n",
        "            softmax_result = F.softmax(values)\n",
        "\n",
        "            class_score = (output[0, indices, 5 + class_idx] * softmax_result).sum()\n",
        "            confidence = (output[0, indices, 4] * softmax_result).sum()\n",
        "            x_c = (output[0, indices, 0] * softmax_result).sum()\n",
        "            y_c = (output[0, indices, 1] * softmax_result).sum()\n",
        "            h = (output[0, indices, 2] * softmax_result).sum()\n",
        "            w = (output[0, indices, 3] * softmax_result).sum()\n",
        "\n",
        "            if self.backprop == 'class':\n",
        "                score = score + class_score\n",
        "            elif self.backprop == 'confidence':\n",
        "                score = score + confidence\n",
        "            elif self.backprop == 'class_confidence':\n",
        "                score = score + confidence * class_score\n",
        "            elif self.backprop == 'x_c':\n",
        "                score = score + x_c\n",
        "            elif self.backprop == 'y_c':\n",
        "                score = score + y_c\n",
        "            elif self.backprop == 'h':\n",
        "                score = score + h\n",
        "            elif self.backprop == 'w':\n",
        "                score = score + w\n",
        "            else:\n",
        "                raise NotImplementedError('Not implemented')\n",
        "\n",
        "        return score\n",
        "\n",
        "def extract_CAM(method, model: torch.nn.Module, predicted_bbox, classes, backward_per_class: bool, image, layer: int,\n",
        "                device, backprop_array, keep_only_topk, crop, negative_crop, use_old_target_method, **kwargs):\n",
        "    if not classes:\n",
        "        classes = predicted_bbox['class'].values\n",
        "\n",
        "    target_layers = [model.model.model[layer]]\n",
        "\n",
        "    bbox_torch = torch.tensor(predicted_bbox.drop('name', axis=1).values.astype(np.float64), device=device)\n",
        "\n",
        "    if not backprop_array:\n",
        "        backprop_array = ['class']\n",
        "\n",
        "    cam_array = []\n",
        "    use_cuda = False\n",
        "\n",
        "    if not backward_per_class:\n",
        "        for item in backprop_array:\n",
        "            if use_old_target_method:\n",
        "                targets = [YOLOBoxScoreTarget(classes=classes)]\n",
        "            else:\n",
        "                targets = [\n",
        "                    YOLOBoxScoreTarget2(predicted_bbox=bbox_torch, backprop=item, classes=classes, device=device)]\n",
        "\n",
        "            cam = method(model, target_layers, use_cuda=use_cuda, reshape_transform=yolo_reshape_transform, **kwargs)\n",
        "            grayscale_cam = cam(image, targets=targets)\n",
        "            grayscale_cam = grayscale_cam[0, :]\n",
        "            cam_array.append(grayscale_cam)\n",
        "    else:\n",
        "        for class_ in classes:\n",
        "            for item in backprop_array:\n",
        "                if use_old_target_method:\n",
        "                    targets = [YOLOBoxScoreTarget(classes=class_)]\n",
        "                else:\n",
        "                    targets = [\n",
        "                        YOLOBoxScoreTarget2(predicted_bbox=bbox_torch, backprop=item, device=device, classes=[class_])]\n",
        "\n",
        "                cam = method(model,\n",
        "                             target_layers,\n",
        "                             use_cuda=use_cuda,\n",
        "                             reshape_transform=yolo_reshape_transform,\n",
        "                             **kwargs)\n",
        "                grayscale_cam = cam(image, targets=targets)\n",
        "                grayscale_cam = grayscale_cam[0, :]\n",
        "                cam_array.append(grayscale_cam)\n",
        "\n",
        "    final_cam = sum(cam_array)\n",
        "\n",
        "    if final_cam.max() > 0: \n",
        "        final_cam = final_cam / final_cam.max() \n",
        "\n",
        "    if 0 < keep_only_topk < 100:\n",
        "        k = np.percentile(final_cam, 100 - keep_only_topk)\n",
        "        indices = np.where(final_cam <= k)\n",
        "        final_cam[indices] = 0\n",
        "\n",
        "    fixed_image = np.array(image[0].cpu()).transpose(1, 2, 0)\n",
        "\n",
        "    threshold = 0.42\n",
        "    cam_image = np.where(final_cam > threshold, 0, final_cam)\n",
        "\n",
        "    if crop:\n",
        "        indices = np.where(final_cam > threshold)\n",
        "        cam_image = fixed_image.copy()\n",
        "        cam_image[indices] = 0\n",
        "        cam_image = cam_image * 255\n",
        "\n",
        "    elif negative_crop:\n",
        "       indices = np.where(final_cam < threshold)\n",
        "       cam_image = fixed_image.copy()\n",
        "       cam_image[indices] = 0\n",
        "       cam_image = cam_image * 255\n",
        "\n",
        "    else:\n",
        "        cam_image = show_cam_on_image(fixed_image, final_cam, use_rgb=True)\n",
        "    return cam_image, final_cam\n",
        "\n",
        "\n",
        "def explain(method: str, raw_model, predicted_bbox, classes, backward_per_class, image, layer: int, device,\n",
        "            backprop_array, keep_only_topk, crop, negative_crop, use_old_target_method):\n",
        "    cam_image = None\n",
        "    method_obj = None\n",
        "    extra_arguments = {}\n",
        "\n",
        "    if method.lower() == 'GradCAM'.lower():\n",
        "        method_obj = GradCAM\n",
        "    elif method.lower() == 'EigenCAM'.lower():\n",
        "        method_obj = EigenCAM\n",
        "    elif method.lower() == 'GradCAMPlusPlus'.lower() \\\n",
        "        or method.lower() == 'GradCAM++'.lower():\n",
        "        method_obj = GradCAMPlusPlus\n",
        "    else:\n",
        "        raise NotImplementedError('The method that you requested has not yet been implemented')\n",
        "\n",
        "    try:\n",
        "        cam_image, heat_map = extract_CAM(method_obj,\n",
        "                                          raw_model,\n",
        "                                          predicted_bbox,\n",
        "                                          classes,\n",
        "                                          backward_per_class,\n",
        "                                          image,\n",
        "                                          layer,\n",
        "                                          device=device,\n",
        "                                          backprop_array=backprop_array,\n",
        "                                          keep_only_topk=keep_only_topk,\n",
        "                                          crop=crop,\n",
        "                                          negative_crop=negative_crop,\n",
        "                                          use_old_target_method=use_old_target_method,\n",
        "                                          **extra_arguments)\n",
        "    except Exception as e:\n",
        "        LOGGER.error(f'{e}')\n",
        "        cam_image = image\n",
        "        heat_map = torch.zeros_like(image)\n",
        "\n",
        "    return cam_image, heat_map\n",
        "\n",
        "\n",
        "class YoloOutputWrapper(DetectMultiBackend):\n",
        "    \"\"\"\n",
        "    Main purpose of using this method is to eliminate the second argument in YOLO output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n",
        "        super().__init__(weights=weights, device=device, dnn=dnn, data=data, fp16=fp16, fuse=fuse)\n",
        "\n",
        "    def forward(self, x):\n",
        "        total_prediction, _ = super().forward(x)\n",
        "        return total_prediction\n",
        "\n",
        "\n",
        "def run(\n",
        "        weights= '/content/drive/MyDrive/yolov5_best.pt',  # model path or triton URL\n",
        "        source= '/content/drive/MyDrive/cam',  # file/dir/URL/glob/screen/0(webcam)\n",
        "        method='GradCAM',  # the method for interpreting the results\n",
        "        layer=-2,\n",
        "        keep_only_topk=100,  # this can be 0 to 1. it shows maximum percentage of pixels\n",
        "        # which can be used for heatmap. This is good for evaluation of heatmaps!\n",
        "        class_names=[],  # list of class names to use for CAM methods\n",
        "        backprop_array=[],  # list of items to do backprop! It can be class, confidence,\n",
        "        backward_per_class=False,  # whether the method should backprop per each class or do it all at one backward\n",
        "        crop=False,\n",
        "        negative_crop=False,\n",
        "        use_old_target_method=False,  # whether to use old target method or new one\n",
        "        imgsz=(800, 800),  # inference size (height, width)\n",
        "        device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        project= '/content/eigencam',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        nosave=False,  # do not save images/videos\n",
        "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        verbose=False,  # verbose output\n",
        "        vid_stride=1,  # video frame-rate stride\n",
        "):\n",
        "    source = str(source)\n",
        "    save_img = not nosave and not source.endswith('.txt')  \n",
        "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
        "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "    if is_url and is_file:\n",
        "        source = check_file(source)  \n",
        "\n",
        "    device = select_device(device)\n",
        "\n",
        "    model = YoloOutputWrapper(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
        "    autoshaped_model = AutoShape(DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half))\n",
        "\n",
        "    stride, pt = model.stride, model.pt\n",
        "    imgsz = check_img_size(imgsz, s=stride) \n",
        "    model.requires_grad_(True)\n",
        "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
        "\n",
        "    model_classes = {v: k for k, v in model.names.items()}\n",
        "    class_idx = [model_classes[item] for item in class_names]\n",
        "\n",
        "    last_image = None  \n",
        "    for path, im, _, _, _ in dataset:\n",
        "        processed_output = autoshaped_model(im)\n",
        "        predicted_bbox = processed_output.pandas().xyxy[0]\n",
        "       \n",
        "\n",
        "        im = torch.from_numpy(im).to(model.device)\n",
        "        im = im.half() if model.fp16 else im.float() \n",
        "        im /= 255  \n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  \n",
        "\n",
        "        _ = model(im)\n",
        "\n",
        "        cam_image, heat_map = explain(method=method,\n",
        "                                      raw_model=model,\n",
        "                                      predicted_bbox=predicted_bbox,\n",
        "                                      classes=class_idx,\n",
        "                                      backward_per_class=backward_per_class,\n",
        "                                      image=im,\n",
        "                                      layer=layer,\n",
        "                                      device=device,\n",
        "                                      backprop_array=backprop_array,\n",
        "                                      keep_only_topk=keep_only_topk,\n",
        "                                      crop=crop,\n",
        "                                      negative_crop=negative_crop,\n",
        "                                      use_old_target_method=use_old_target_method)\n",
        "        path = Path(path)\n",
        "        save_path = str(save_dir / path.name)  # im.jpg\n",
        "        bgr_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(save_path, bgr_image)\n",
        "\n",
        "        cv2.imwrite(save_path.replace(path.suffix, '_heat_' + path.suffix), 255-(heat_map * 255))\n",
        "        LOGGER.info(f'saved image to {save_path}')\n",
        "        last_image = cam_image\n",
        "\n",
        "    return last_image\n",
        "\n",
        "def parseopt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default= 'yolov5x.pt', help='model path or triton URL')\n",
        "    parser.add_argument('--source', type=str, default='data/images', help='file/dir/URL/glob/screen/0(webcam)')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[800], help='inference size h,w')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--method', type=str, default='EigenCAM', help='the method to use for interpreting the feature maps')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--project', default= 'runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
        "    parser.add_argument('--verbose', action='store_true', help='verbose log')\n",
        "    parser.add_argument('--layer', type=int, default=-2, help='layer to backpropagate gradients to')\n",
        "    parser.add_argument('--class-names', nargs='*', default='', help='filter by class: --classes dog, or --classes dog cat')\n",
        "    parser.add_argument('--keep-only-topk', type=int, default=100, help='percentage of heatmap pixels to keep')\n",
        "    parser.add_argument('--backprop-array', nargs='*', default='', help='backprop array items')\n",
        "    parser.add_argument('--backward-per-class', type=bool, default=False, help='whether the method should backprop per each class or do it all at one backward')\n",
        "    parser.add_argument('--crop', type=bool, default=False, help='use this if you want to crop heatmap area in order to evaluate methods for interpretability')\n",
        "    parser.add_argument('--negative_crop', type=bool, default=False, help='use this if you want to crop heatmap area in order to evaluate methods for interpretability')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "-oWG8qyqqxaS",
        "outputId": "67547965-e52c-46f4-bd26-68a484e55900"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/drive/MyDrive/yolov5_best.pt')\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/cam'\n",
        "image_files = [os.path.join(image_directory, file) for file in os.listdir(image_directory) if file.endswith('.jpg')]\n",
        "\n",
        "for img_path in image_files:\n",
        "    results = model(img_path)\n",
        "    image = run(source=img_path, method='EigenCAM', layer=9, class_names=['storagetank', 'baseballfield', 'tenniscourt', 'basketballcourt',\n",
        "                                                                               'windmill', 'vehicle', 'harbor', 'ship', 'airplane', 'bridge', 'overpass',\n",
        "                                                                               'Expressway-toll-station', 'trainstation', 'chimney', 'groundtrackfield',\n",
        "                                                                               'dam', 'Expressway-Service-area', 'stadium', 'airport', 'golffield'],\n",
        "                weights='/content/drive/MyDrive/yolov5_best.pt', negative_crop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YAnw-VveEtV",
        "outputId": "1225976c-7359-4a54-a802-966b47933e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_config.yaml\n",
        "\n",
        "path: '/content/drive/MyDrive/test'\n",
        "train:\n",
        "val: '/content/drive/MyDrive/test/images/cam'\n",
        "names:\n",
        "  0: Expressway-Service-area\n",
        "  1: Expressway-toll-station\n",
        "  2: airplane\n",
        "  3: airport\n",
        "  4: baseballfield\n",
        "  5: basketballcourt\n",
        "  6: bridge\n",
        "  7: chimney\n",
        "  8: dam\n",
        "  9: golffield\n",
        "  10: groundtrackfield\n",
        "  11: harbor\n",
        "  12: overpass\n",
        "  13: ship\n",
        "  14: stadium\n",
        "  15: storagetank\n",
        "  16: tenniscourt\n",
        "  17: trainstation\n",
        "  18: vehicle\n",
        "  19: windmill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pBJFm8PeYkA",
        "outputId": "76455966-4ecf-4871-cdf3-f5505f82bb08"
      },
      "outputs": [],
      "source": [
        "!python /content/yolov5/val.py --weights /content/drive/MyDrive/yolov5_best.pt --data /content/test_config.yaml --imgsz 800"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLy6ebNlL34L"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
